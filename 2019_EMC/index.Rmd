---
title: "Validating Predictions from GEEs"
author: "Dimitris Rizopoulos"
date: "January 29, 2020"
output:
  ioslides_presentation:
    css: pres.css
    widescreen: yes
    mathjax: default
    logo: emc.png
transition: none
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
library("cvGEE")
library("geepack")
library("splines")
library("lattice")
```

# Motivation

## Grouped Categorical Data

<br/>

- Often interest in grouped non-Gaussian data
    + dichotomous/binomial
    + counts
    + ordinal
    + ...

## Grouped Categorical Data (cont'd)

- For normal data we have the multivariate version of the normal distribution
  + equivalance between marginal & mixed effects models

<br/>

- For categorical data
  + important differences between marginal and mixed models

## Grouped Categorical Data (cont'd)

- We focus on marginal models
  + multivariate distributions for categorical data (e.g., Dale, copulas, etc.)
  + <font color="red">*difficult to fit*</font>

<br/>
  
- Liang and Zeger have come to the rescue
  
<div style="text-align:center;width:700px;border:3px solid black">
<br/>
<font color = "red" size = "6">Genaralized Estimating Equations (GEEs)</font>
<br/>
</div>

## GEEs

<br/>

- GEEs have seen considerable development the last 20-30 years
  + missing data
  + causal inference
  + measurement error
  + ...

## GEEs (cont'd)

- Let $Y_i$ denote a multivariate outcome with marginal mean $$\mu_i = g(X_i^\top \beta)$$

<br/>

- Estimates for $\beta$ are obtained by solving the equations
$$S(\beta) = \sum_i \frac{\partial \mu_i}{\partial \beta } \; {\color{red} V_i^{-1}} \; (Y_i - \mu_i) = 0$$

## GEEs (cont'd)

- The trick is in the definition of ${\color{red} V_i^{-1}}$
$${\color{red} V_i^{-1}} = A_i^{1/2} \; R_i(\alpha) \; A_i^{1/2}$$
where
  + $A_i= \mbox{diag}\{\mbox{var}(Y_i)^{1/2}\}$ a diagonal matrix with standard deviations
  + $R_i(\alpha)$ a *working* assumption for the pairwise correlations

## GEEs (cont'd)

- If the assumed mean structure $\mu_i$ is correctly specified, then $$\widehat{\beta} \sim \mathcal N \{ \mathbf{\beta}, \mbox{var}(\widehat{\beta})\}$$
where $$\mbox{var}(\widehat{\beta}) = V_0^{-1} V_1 V_0^{-1}$$ 
with 
  + $V_0 = \sum_{i} \frac{\partial \mu_i}{\partial \beta^\top} V_i^{-1} \frac{\partial \mu_i}{\partial\beta}$
  + $V_1 = \sum_{i} \frac{\partial \mu_i}{\partial \beta^\top} V_i^{-1} \mbox{var}(Y_i) V_i^{-1} \frac{\partial\mu_i}{\partial \beta}$

## GEEs (cont'd)

<br/>

- For a dichotomous outcome
  + the mean is $$\mu_i = \pi_i = \frac{\exp(X_i^\top \beta)}{1 + \exp(X_i^\top \beta)}$$
  + the variance accounts for over-dispersion using the quasi-likelihood formulation
  $$\mbox{var}(Y_i) = {\color{red} \phi} \pi_i (1 - \pi_i)$$

## Predictions from GEEs

- A CPO project in which we fitted a GEE for count data

<br/>

- Two questions
  + Estimate $\beta$ and report results
  + calculate and <font color="red">*validate*</font> predictions

## Predictions from GEEs (cont'd)

- Predictions from a GEE are simply $$\widehat{\mu}_{ij} = g(X_{ij} \widehat{\beta})$$

<br/>

- But how to <font color="red">*properly*</font> validate these predictions?

## Predictions from GEEs (cont'd)

- We could calculate the prediction error $$\sum_i\sum_j (Y_{ij} - \widehat{\mu}_{ij})^2$$

<br/>

- But this is not a scoring rule
  + we only look at the mean <font color="red">*not*</font> the whole distribution
  
## Proper Scoring Rules

- Let $Y_{ij} \in \{1, \ldots, K\}$ a categorical outcome
  + $K$ finite (e.g., binary data)
  + $K$ infinite (e.g., count data)
  
<br/>

- Let also $\pi_{ijk} = \Pr(Y_{ij} = k)$, $k = 1, \ldots, K$

## Proper Scoring Rules (cont'd)

- A scoring rule $$R(\pi_{ij}, \{Y_{ij} = k\})$$ a function that assigns a score for the forecast $\{\pi_{ijk}; k = 1, \ldots, K\}$ upon observing the outcome $\{Y_{ij} = k\}$

<br/>

- Based on observing $Y_{ij}$, we get probabilities for all $K$ categories

## Proper Scoring Rules (cont'd)

<br/>

<div style="float:center;text-align:center;width:80%;border:3px solid black">
<br/><br/>
<strong><font size="6" color = "red">A proper scoring rule is a rule that is maximized when the true probabilities are used in its calculation</font></strong>
<br/><br/>
</div>

## Proper Scoring Rules (cont'd)

- The most well-known proper scoring rules
  + Logarithmic: $$R(\pi_{ij}, \{Y_{ij} = k\}) = \log(\pi_{ijk}), \quad (-\infty, 0]$$
  
  <br/>
  
  + Quadratic: $$R(\pi_{ij}, \{Y_{ij} = k\}) = 2\pi_{ijk} - \sum_{\ell = 1}^K \pi_{ij\ell}^2, \quad [-1, 1]$$

## Proper Scoring Rules (cont'd)

- The most well-known proper scoring rules
  + Spherical: $$R(\pi_{ij}, \{Y_{ij} = k\}) = \pi_{ijk} \Bigg / \sqrt{\sum_{\ell = 1}^K \pi_{ij\ell}^2}, \quad [0, 1]$$

## Proper Scoring Rules (cont'd)

- However, in GEEs no specification of the distribution of the data
  + we only define the first two moments

<br/>

<div style="float:center;text-align:center;width:80%;border:3px solid black">
<br/><br/>
<strong><font size="6" color = "red">We do not get a probability mass function</font></strong>
<br/><br/>
</div>

## Proper Scoring Rules (cont'd)

- We make the following conventions
  + For binary data we use the Bernoulli distribution
  
  <br/>
  
  + For binomial data we use the Beta-Binomial distribution
    + quasi-likelihood $\mbox{var}(Y_{ij}) = \phi K \pi_{ijk} (1 - \pi_{ijk})$
    + scale $\theta$ of Beta-Binomial is set to $$(K - \phi) / (\phi - 1)$$

## Proper Scoring Rules (cont'd)

<br/>

- We make the following conventions
  + For count data we use the Negative Binomial distribution
    + quasi-likelihood $\mbox{var}(Y_{ij}) = \phi \mu_{ij}$
    + shape $\theta$ of Negative Binomial is set to $$\mu_{ij} / (\phi - 1)$$

## Implementation

<br/>

<div style="float:center;text-align:center;width:60%;border:3px solid black">
<br/><br/>
<strong><font size="6" color = "red">R Package cvGEE</font></strong>
<br/><br/>
</div>

<br/>

- Calculation of proper scoring rules using cross-validation

## Implementation (cont'd)

- Basic function `cv_gee()` with arguments
  + `object` a model object from **geepack**
  + `rule` which scoring rule to calculate
  + `K` number of folds in CV
  + `M` how many times to repeat CV
  + `seed` the seed used in the calculation
  + `return_data` return results in a data frame

## Example AIDS

```{r AIDS_analysis_models, message = FALSE}
aids$CD4count <- aids$CD4 * aids$CD4
aids$obstimef <- factor(aids$obstime)

fm1 <- geeglm(CD4count ~ obstimef * drug, family = poisson(), data = aids, 
              id = patient, corstr = "independence")

fm2 <- update(fm1, corstr = "exchangeable")

fm3 <- update(fm1, corstr = "ar1")
```

## Example AIDS (cont'd)

```{r AIDS_analysis_QIC, message = FALSE}
t(sapply(list("independence" = fm1, "exchangeable" = fm2, "ar1" = fm3), QIC))
```

## Example AIDS (cont'd)

```{r AIDS_analysis_cvGEE, message = FALSE}
plot_data <- cv_gee(fm1, return_data = TRUE, max_count = 1000)
plot_data$independence <- plot_data$.score

plot_data$exchangeable <- unlist(cv_gee(fm2, max_count = 1000))

plot_data$ar1 <- unlist(cv_gee(fm3, max_count = 1000))
```

## Example AIDS (cont'd)

```{r AIDS_analysis_plot, message = FALSE, echo = FALSE}
xyplot(independence + exchangeable + ar1 ~ obstime | .rule, 
       data = plot_data, type = c("p", "smooth"), auto.key = TRUE, 
       layout = c(3, 1), xlab = "Follow-up time (months)", ylab = "Scoring Rules",
       scales = list(y = list(relation = "free")), pch = ".", lwd = 2)
```

## Example PBC

```{r PBC_analysis_models, message = FALSE}
pbc2$spiders2 <- as.numeric(pbc2$spiders == "Yes")
fm1 <- geeglm(spiders2 ~ year + drug + age + sex,
              family = binomial(), data = pbc2, id = id, 
              corstr = "independence")

fm2 <- update(fm1, formula = spiders2 ~ year + drug + age * sex)

fm3 <- update(fm1, spiders2 ~ year * drug + ns(age, 2) * sex)
```

## Example PBC (cont'd)

```{r PBC_analysis_QIC, message = FALSE}
t(sapply(list("additive" = fm1, "interaction_I" = fm2, "interaction_II" = fm3), QIC))
```

## Example PBC (cont'd)

```{r PBC_analysis_cvGEE, message = FALSE}
plot_data <- cv_gee(fm1, return_data = TRUE)
plot_data$additive <- plot_data$.score

plot_data$interaction_I <- unlist(cv_gee(fm2))

plot_data$interaction_II <- unlist(cv_gee(fm3))
```

## Example PBC (cont'd)

```{r PBC_analysis_plot, message = FALSE, echo = FALSE}
xyplot(additive + interaction_I + interaction_II ~ year | .rule, data = plot_data, 
       type = c("p", "smooth"), auto.key = TRUE, pch = ".", lwd = 3,
       scales = list(y = list(relation = "free")), layout = c(3, 1),
       xlab = "Follow-up time (years)", ylab = "Scoring Rules")
```

## Discussion

- Should we look at the mean or whole distribution?
  + former inline with GEE specification
  + latter gives a more complete picture

<br/>

- cvGEE
  + Dedicated website: [https://drizopoulos.github.io/cvGEE/](https://drizopoulos.github.io/cvGEE/)

***

<br/> 
<br/> 

<div align = "center">
<font color = "black" size = "6">**Thank you for your attention!**</font>
</div>
<br/>
<div align = "center">
[http://www.drizopoulos.com/](http://www.drizopoulos.com/)
</div>

<br/> 
<br/>
<br/>

<div align = "center">
<font color = "black" size = "5">**These slides are available at:**</font>
</div>
<div align = "center">
<div align = "center">
[https://drizopoulos.github.io/cvGEE_presentations/](https://drizopoulos.github.io/cvGEE_presentations/)
</div>

